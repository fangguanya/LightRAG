import sys

if sys.version_info < (3, 9):
    pass
else:
    pass
import pipmaster as pm  # Pipmaster for dynamic library install

# install specific modules
if not pm.is_installed("lmdeploy"):
    pm.install("lmdeploy")

from openai import (
    APIConnectionError,
    RateLimitError,
    APITimeoutError,
)
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)


import numpy as np
import aiohttp
import base64
import struct
import logging
import asyncio
import time
from typing import List, Optional
import math

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

# å…¨å±€è¯·æ±‚é¢‘ç‡æ§åˆ¶å™¨
class RateLimiter:
    def __init__(self, rpm: int = 5000, tpm: int = 1000000):
        self.rpm = rpm  # æ¯åˆ†é’Ÿè¯·æ±‚æ•°é™åˆ¶
        self.tpm = tpm  # æ¯åˆ†é’Ÿtokenæ•°é™åˆ¶
        self.request_times = []  # è¯·æ±‚æ—¶é—´è®°å½•
        self.token_usage = []   # tokenä½¿ç”¨è®°å½•
        self.lock = asyncio.Lock()
    
    def estimate_tokens(self, texts: List[str]) -> int:
        """é¢„ä¼°æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼š1ä¸ªtokençº¦ç­‰äº4ä¸ªå­—ç¬¦ï¼‰"""
        total_chars = sum(len(text) for text in texts)
        return max(1, total_chars // 4)
    
    async def acquire(self, texts: List[str]):
        """è·å–è¯·æ±‚è®¸å¯ï¼Œç¡®ä¿ä¸è¶…è¿‡é¢‘ç‡é™åˆ¶"""
        async with self.lock:
            current_time = time.time()
            estimated_tokens = self.estimate_tokens(texts)
            
            # æ¸…ç†1åˆ†é’Ÿå‰çš„è®°å½•
            cutoff_time = current_time - 60
            self.request_times = [t for t in self.request_times if t > cutoff_time]
            self.token_usage = [(t, tokens) for t, tokens in self.token_usage if t > cutoff_time]
            
            # æ£€æŸ¥RPMé™åˆ¶
            if len(self.request_times) >= self.rpm:
                wait_time = 60 - (current_time - self.request_times[0])
                if wait_time > 0:
                    #logger.info(f"RPMé™åˆ¶ï¼Œç­‰å¾… {wait_time:.2f} ç§’")
                    await asyncio.sleep(wait_time)
                    return await self.acquire(texts)
            
            # æ£€æŸ¥TPMé™åˆ¶
            current_tokens = sum(tokens for _, tokens in self.token_usage)
            if current_tokens + estimated_tokens > self.tpm:
                if self.token_usage:
                    wait_time = 60 - (current_time - self.token_usage[0][0])
                    if wait_time > 0:
                        #logger.info(f"TPMé™åˆ¶ï¼Œå½“å‰tokens: {current_tokens}, é¢„ä¼°éœ€è¦: {estimated_tokens}, ç­‰å¾… {wait_time:.2f} ç§’")
                        await asyncio.sleep(wait_time)
                        return await self.acquire(texts)
            
            # è®°å½•æœ¬æ¬¡è¯·æ±‚
            self.request_times.append(current_time)
            self.token_usage.append((current_time, estimated_tokens))
            
            # æ·»åŠ åŸºç¡€é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡äºå¯†é›†
            await asyncio.sleep(0.1)

# å…¨å±€é¢‘ç‡é™åˆ¶å™¨å®ä¾‹
rate_limiter = RateLimiter(rpm=4000, tpm=800000)  # è®¾ç½®ä¸ºé™åˆ¶çš„80%ï¼Œç•™å‡ºå®‰å…¨è¾¹é™…


@retry(
    stop=stop_after_attempt(5),  # å¢åŠ é‡è¯•æ¬¡æ•°
    wait=wait_exponential(multiplier=2, min=4, max=120),  # å¢åŠ æœ€å¤§ç­‰å¾…æ—¶é—´
    retry=retry_if_exception_type(
        (RateLimitError, APIConnectionError, APITimeoutError)
    ),
)
async def siliconcloud_embedding(
    texts: list[str],
    model: str = "BAAI/bge-m3",
    base_url: str = "https://api.siliconflow.cn/v1/embeddings",
    max_token_size: int = 8192,
    api_key: str = None,
    batch_size: Optional[int] = None,
) -> np.ndarray:
    if not texts:
        return np.array([])
    
    # è‡ªåŠ¨ç¡®å®šæ‰¹æ¬¡å¤§å°
    if batch_size is None:
        # æ ¹æ®æ–‡æœ¬é•¿åº¦å’Œtokené™åˆ¶åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
        avg_text_length = sum(len(text) for text in texts) / len(texts)
        estimated_tokens_per_text = max(1, avg_text_length // 4)
        # ç¡®ä¿å•æ‰¹æ¬¡ä¸è¶…è¿‡TPMé™åˆ¶çš„10%
        max_batch_tokens = rate_limiter.tpm // 10
        batch_size = max(1, min(50, max_batch_tokens // estimated_tokens_per_text))
        #logger.info(f"ğŸ”§ è‡ªåŠ¨ç¡®å®šæ‰¹æ¬¡å¤§å°: {batch_size}, é¢„ä¼°æ¯æ–‡æœ¬tokenæ•°: {estimated_tokens_per_text}")
    
    # åˆ†æ‰¹å¤„ç†å¤§é‡æ–‡æœ¬
    if len(texts) > batch_size:
        total_batches = math.ceil(len(texts) / batch_size)
        #logger.info(f"ğŸ“¦ æ–‡æœ¬æ•°é‡ {len(texts)} è¶…è¿‡æ‰¹æ¬¡å¤§å° {batch_size}ï¼Œå°†åˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡å¤„ç†")
        
        all_embeddings = []
        start_time = time.time()
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            current_batch = i // batch_size + 1
            
            # è®¡ç®—æ€»ä½“è¿›åº¦
            overall_progress = (current_batch - 1) / total_batches * 100
            #logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {current_batch}/{total_batches} ({overall_progress:.1f}%) - åŒ…å« {len(batch_texts)} ä¸ªæ–‡æœ¬")
            
            batch_start_time = time.time()
            batch_embeddings = await siliconcloud_embedding(
                batch_texts, model, base_url, max_token_size, api_key, batch_size
            )
            batch_end_time = time.time()
            
            all_embeddings.extend(batch_embeddings)
            
            # è®¡ç®—å®Œæˆè¿›åº¦å’Œå‰©ä½™æ—¶é—´ä¼°ç®—
            completed_progress = current_batch / total_batches * 100
            elapsed_time = time.time() - start_time
            if current_batch > 1:
                avg_time_per_batch = elapsed_time / current_batch
                remaining_batches = total_batches - current_batch
                estimated_remaining_time = avg_time_per_batch * remaining_batches
                #logger.info(f"âœ… æ‰¹æ¬¡ {current_batch} å®Œæˆ ({completed_progress:.1f}%) - è€—æ—¶ {batch_end_time - batch_start_time:.2f}s, é¢„è®¡å‰©ä½™æ—¶é—´ {estimated_remaining_time:.1f}s")
            # else:
                #logger.info(f"âœ… æ‰¹æ¬¡ {current_batch} å®Œæˆ ({completed_progress:.1f}%) - è€—æ—¶ {batch_end_time - batch_start_time:.2f}s")
        
        total_time = time.time() - start_time
        #logger.info(f"ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼æ€»è®¡å¤„ç† {len(texts)} ä¸ªæ–‡æœ¬ï¼Œå…± {total_batches} ä¸ªæ‰¹æ¬¡ï¼Œæ€»è€—æ—¶ {total_time:.2f}s")
        return np.array(all_embeddings)
    
    # è¯·æ±‚é¢‘ç‡æ§åˆ¶
    await rate_limiter.acquire(texts)
    
    if api_key and not api_key.startswith("Bearer "):
        api_key = "Bearer " + api_key

    headers = {"Authorization": api_key, "Content-Type": "application/json"}

    truncate_texts = [text[0:max_token_size] for text in texts]

    payload = {"model": model, "input": truncate_texts, "encoding_format": "base64"}
    
    #logger.info(f"ğŸ“¡ å‘é€APIè¯·æ±‚ - å¤„ç† {len(texts)} ä¸ªæ–‡æœ¬")

    base64_strings = []
    async with aiohttp.ClientSession() as session:
        async with session.post(base_url, headers=headers, json=payload) as response:
            # ç‰¹æ®Šå¤„ç†429é”™è¯¯
            if response.status == 429:
                response_text = await response.text()
                logger.warning(f"âš ï¸ é‡åˆ°429é™æµé”™è¯¯: {response_text}")
                # è§£æå“åº”è·å–å»ºè®®ç­‰å¾…æ—¶é—´
                try:
                    error_content = await response.json() if response.content_type == 'application/json' else {}
                    if 'retry_after' in error_content:
                        wait_time = int(error_content['retry_after'])
                    else:
                        wait_time = 60  # é»˜è®¤ç­‰å¾…60ç§’
                except:
                    wait_time = 60
                
                #logger.info(f"â³ ç”±äº429é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•")
                await asyncio.sleep(wait_time)
                raise RateLimitError(f"Rate limit exceeded: {response_text}")
            
            # æ£€æŸ¥å…¶ä»–HTTPé”™è¯¯çŠ¶æ€ç 
            if response.status != 200:
                response_text = await response.text()
                error_msg = f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}, å“åº”å†…å®¹: {response_text}"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            try:
                content = await response.json()
            except Exception as e:
                response_text = await response.text()
                error_msg = f"è§£æJSONå“åº”å¤±è´¥: {str(e)}, åŸå§‹å“åº”: {response_text}"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            # è®°å½•å®Œæ•´çš„å“åº”å†…å®¹ç”¨äºè°ƒè¯•
            #logger.debug(f"APIå“åº”å†…å®¹: {content}")
            
            # æ£€æŸ¥APIè¿”å›çš„é”™è¯¯ç 
            if "code" in content:
                error_msg = f"APIè¿”å›é”™è¯¯ç : {content}"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            # æ£€æŸ¥dataå­—æ®µæ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºNone
            if "data" not in content:
                error_msg = f"APIå“åº”ä¸­ç¼ºå°‘'data'å­—æ®µï¼Œå®Œæ•´å“åº”: {content}"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            if content["data"] is None:
                error_msg = f"APIå“åº”ä¸­'data'å­—æ®µä¸ºNoneï¼Œå®Œæ•´å“åº”: {content}"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            if not isinstance(content["data"], list):
                error_msg = f"APIå“åº”ä¸­'data'å­—æ®µä¸æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œç±»å‹: {type(content['data'])}, å®Œæ•´å“åº”: {content}"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            if len(content["data"]) == 0:
                error_msg = f"APIå“åº”ä¸­'data'å­—æ®µä¸ºç©ºåˆ—è¡¨ï¼Œå®Œæ•´å“åº”: {content}"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            # éªŒè¯æ¯ä¸ªæ•°æ®é¡¹æ˜¯å¦åŒ…å«embeddingå­—æ®µ
            for i, item in enumerate(content["data"]):
                if not isinstance(item, dict):
                    error_msg = f"data[{i}]ä¸æ˜¯å­—å…¸æ ¼å¼ï¼Œç±»å‹: {type(item)}, å†…å®¹: {item}"
                    logger.error(error_msg)
                    raise ValueError(error_msg)
                
                if "embedding" not in item:
                    error_msg = f"data[{i}]ç¼ºå°‘'embedding'å­—æ®µï¼Œå†…å®¹: {item}"
                    logger.error(error_msg)
                    raise ValueError(error_msg)
            
            base64_strings = [item["embedding"] for item in content["data"]]
    
    #logger.info(f"ğŸ“¥ æ”¶åˆ°APIå“åº”ï¼Œå¼€å§‹è§£ç  {len(base64_strings)} ä¸ªembedding")

    embeddings = []
    total_embeddings = len(base64_strings)
    decode_start_time = time.time()
    
    for i, string in enumerate(base64_strings):
        try:
            decode_bytes = base64.b64decode(string)
            n = len(decode_bytes) // 4
            float_array = struct.unpack("<" + "f" * n, decode_bytes)
            embeddings.append(float_array)
            
            # æ¯å¤„ç†10%æˆ–è€…æ¯10ä¸ªæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ï¼ˆå–è¾ƒå°å€¼ï¼‰
            progress_interval = max(1, min(10, total_embeddings // 10))
            if (i + 1) % progress_interval == 0 or i == total_embeddings - 1:
                progress = (i + 1) / total_embeddings * 100
                elapsed = time.time() - decode_start_time
                if i > 0:
                    estimated_total = elapsed * total_embeddings / (i + 1)
                    remaining = estimated_total - elapsed
                    #logger.info(f"ğŸ”„ è§£ç è¿›åº¦: {i + 1}/{total_embeddings} ({progress:.1f}%) - å·²ç”¨æ—¶ {elapsed:.2f}s, é¢„è®¡å‰©ä½™ {remaining:.1f}s")
                # else:
                    #logger.info(f"ğŸ”„ è§£ç è¿›åº¦: {i + 1}/{total_embeddings} ({progress:.1f}%)")
                    
        except Exception as e:
            error_msg = f"è§£ç ç¬¬{i}ä¸ªembeddingæ—¶å¤±è´¥: {str(e)}, base64å­—ç¬¦ä¸²: {string[:100]}..."
            logger.error(error_msg)
            raise ValueError(error_msg)
    
    decode_total_time = time.time() - decode_start_time
    #logger.info(f"âœ¨ æˆåŠŸå¤„ç† {len(embeddings)} ä¸ªæ–‡æœ¬çš„embedding - è§£ç æ€»è€—æ—¶ {decode_total_time:.2f}s")
    return np.array(embeddings)
